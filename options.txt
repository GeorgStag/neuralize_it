

activation functions:
exponential, elu, gelu , relu, selu, 
linear, sigmoid, softmax, hard_sigmoid,
softplus, softsign, swish, tanh


optimizers: 
Adadelta, Adagrad, Adam, Adamax, 
Ftrl, Nadam, Optimizer, RMSprop, SGD


loss functions:
mae, mse, logcosh, Huber
Hinge, SquaredHinge, KLDivergence


metrics: 
mae, mse